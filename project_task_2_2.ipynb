{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbbd5b33-09a3-4681-84ea-10e5ed385b76",
   "metadata": {},
   "source": [
    "# 1 Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "591c6e30-bf56-4a3a-a0ab-ccf0afa5663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/kate/spark-3.3.3-bin-hadoop3')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f3fac8f-a3d9-411e-bdd7-132f8ab4a956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a146243-3dc0-45e9-b264-d4bf01c364c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0081708e-ac67-478a-ad81-2b1098246ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyspark\n",
      "Version: 3.3.3\n",
      "Summary: Apache Spark Python API\n",
      "Home-page: https://github.com/apache/spark/tree/master/python\n",
      "Author: Spark Developers\n",
      "Author-email: dev@spark.apache.org\n",
      "License: http://www.apache.org/licenses/LICENSE-2.0\n",
      "Location: /home/kate/spark-3.3.3-bin-hadoop3/python\n",
      "Requires: py4j\n",
      "Required-by: delta-spark\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19c74e4e-2a04-41c1-8851-63e6961c8d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from delta import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c0d07a9-b2a5-42a1-a8e1-89e1b4b5a981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7db4c90-4649-4f3f-ae64-3dba99d33fc7",
   "metadata": {},
   "source": [
    "# 2 Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fb2ef1f-2d98-4311-8131-405fc298ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    " #если нужен запуск до функции\n",
    "#from delta import *\n",
    "\n",
    "#builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "#    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "#    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "#spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fc3f30d-d52b-42d7-90e6-35ba1a0c7106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mirror(directory, name_table, unique_key):\n",
    "    mirror_path = \"task_2.2/mirror\"\n",
    "    output_path = \"task_2.2/mirr_md_account_d\"\n",
    "    log_file_path = \"task_2.2/mirror/my_log.csv\" \n",
    "    out = None\n",
    "    mirror = None\n",
    "    name_table_path = name_table + '*'\n",
    "    name_table_change = None\n",
    "    merge_condition = \" AND \".join([f\"del1.{key} = del2.{key}\" for key in unique_key])\n",
    "    \n",
    "    #открываем спарк сессию\n",
    "    builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "    \n",
    "    #рекурсивно перебираем файлы в директории\n",
    "    for root, dirs, files in sorted(os.walk(directory)):\n",
    "        for name in sorted(files):\n",
    "            if fnmatch.fnmatch(name, name_table_path): #матч имени таблицы\n",
    "                delta_file_path = os.path.join(root, name)\n",
    "                time_start = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                delta_df = spark.read.csv(delta_file_path, header=True, sep=';', inferSchema=True)\n",
    "\n",
    "                if not os.path.exists(log_file_path): #проверяем существование пути\n",
    "                    delta_df.write.format(\"delta\").mode(\"overwrite\").save(mirror_path) #запись дата фрейма в формат дельта\n",
    "                    mirror = DeltaTable.forPath(spark, mirror_path) #создаем дельта таблицу\n",
    "                    time_end = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    out = spark.read.format(\"delta\").load(mirror_path)\n",
    "                    out.show()\n",
    "                    name_table_change = log_delta_processed(time_start, time_end, mirror, delta_file_path, log_file_path)\n",
    "\n",
    "                else:\n",
    "                    if not check_table_name(log_file_path, delta_file_path): #проверяем наличие таблицы в логах\n",
    "                        if mirror is None:\n",
    "                            mirror = DeltaTable.forPath(spark, mirror_path)\n",
    "                            #соединение таблиц по ключу\n",
    "                            mirror.alias(\"del1\").merge(  \n",
    "                               source=delta_df.alias(\"del2\"),\n",
    "                               condition=merge_condition\n",
    "                           ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "                            time_end = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                            out = spark.read.format(\"delta\").load(mirror_path)\n",
    "                            out.show()\n",
    "                            name_table_change = log_delta_processed(time_start, time_end, mirror, delta_file_path, log_file_path)\n",
    "\n",
    "                        else:\n",
    "                            mirror.alias(\"del1\").merge(\n",
    "                               source=delta_df.alias(\"del2\"),\n",
    "                               condition=merge_condition\n",
    "                           ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "                            time_end = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                            out = spark.read.format(\"delta\").load(mirror_path)\n",
    "                            out.show()\n",
    "                            name_table_change = log_delta_processed(time_start, time_end, mirror, delta_file_path, log_file_path)\n",
    "\n",
    "\n",
    "    if out is not None:\n",
    "        out.write.csv(output_path, header=True,mode=\"overwrite\") #запись в итоговый файл\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "def check_table_name(log_file_path, delta_file_path):\n",
    "    if os.path.exists(log_file_path):\n",
    "        with open(log_file_path, \"r\") as log_file:\n",
    "            csvreader = csv.reader(log_file)\n",
    "            for row in csvreader:\n",
    "                if delta_file_path.split(\"/\")[-1].split(\".\")[-2] in row:\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "def log_delta_processed(time_start, time_end, mirror, delta_file_path, log_file_path):\n",
    "    delta_id = delta_file_path.split(\"/\")[-2]\n",
    "    name_table_change = delta_file_path.split(\"/\")[-1].split(\".\")[-2]\n",
    "    log_message = [name_table_change, delta_id, time_start, time_end]\n",
    "    header=[\"table_name\", \"delta_id\", \"time_start\", \"time_end\"] \n",
    "    \n",
    "    if not os.path.exists(log_file_path):\n",
    "        with open(log_file_path, \"w\", newline='') as write_file:\n",
    "            csv_writer = csv.writer(write_file)\n",
    "            csv_writer.writerow(header)\n",
    "            csv_writer.writerow(log_message)\n",
    "    else:\n",
    "        with open(log_file_path, \"a\", newline='') as write_file:\n",
    "            csv_writer = csv.writer(write_file)\n",
    "            csv_writer.writerow(log_message)\n",
    "\n",
    "    return name_table_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "672c2549-3fc3-405a-a80a-3af092efb58b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/18 14:15:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/09/18 14:15:35 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/09/18 14:15:37 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build left for full outer join.\n",
      "23/09/18 14:15:37 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build left for full outer join.\n",
      "23/09/18 14:15:37 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build left for full outer join.\n",
      "+----------------+--------------------+----------+--------------------+---------+-----------+-------------+---------+---------+----------------+\n",
      "|DATA_ACTUAL_DATE|DATA_ACTUAL_END_DATE|ACCOUNT_RK|      ACCOUNT_NUMBER|CHAR_TYPE|CURRENCY_RK|CURRENCY_CODE|CLIENT_ID|BRANCH_ID|OPEN_IN_INTERNET|\n",
      "+----------------+--------------------+----------+--------------------+---------+-----------+-------------+---------+---------+----------------+\n",
      "|      15.02.2018|          31.12.2050|     13560|30110810300000008001|        A|         34|          643|       20|      105|               Y|\n",
      "|      21.04.2018|          31.12.2050|     13630|30102810900000002185|        A|         34|          643|       21|      107|               N|\n",
      "|      21.04.2018|          31.12.2050|     13811|30221978100000008100|        A|         44|          978|       33|      201|            null|\n",
      "|      21.04.2018|          31.12.2050|     13871|30222978200000004100|        P|         44|          978|       63|      105|               Y|\n",
      "|      01.01.2018|          31.12.2050|     13904|30204810500000002001|        A|         34|          643|       37|      201|               Y|\n",
      "|      21.04.2018|          31.12.2050|     13905|30202810900000002001|        A|         34|          643|       12|      107|            null|\n",
      "|      01.01.2018|          31.12.2050|     13906|30114756800000051003|        A|         30|          756|       40|      105|            null|\n",
      "|      15.02.2018|          31.12.2050|     14136|30220826800838890001|        P|         31|          826|       57|      107|            null|\n",
      "|      01.01.2018|          31.12.2050|     14138|30220978800838890001|        P|         44|          978|       24|      105|               Y|\n",
      "|      01.01.2018|          31.12.2050|     17132|30111810000000666001|        P|         34|          643|       19|      101|            null|\n",
      "|      15.02.2018|          31.12.2050|     17244|30111810900000672001|        P|         34|          643|       99|      127|            null|\n",
      "|      01.01.2018|          31.12.2050|     17434|30111810900000051004|        P|         34|          643|        9|      101|            null|\n",
      "|      10.03.2018|          31.12.2050|     17439|30111810500000438001|        P|         34|          643|        4|      203|            null|\n",
      "|      10.03.2018|          31.12.2050|     17442|30111810800000565001|        P|         34|          810|       82|      107|               N|\n",
      "|      10.03.2018|          31.12.2050|     18006|30233978800450003001|        A|         44|          978|       71|      201|            null|\n",
      "|      10.03.2018|          31.12.2050|     18164|30232810000450001001|        P|         34|          810|       90|      103|            null|\n",
      "|      21.04.2018|          31.12.2050|     18165|30232978900450001001|        P|         44|          978|       13|      120|               N|\n",
      "|      01.01.2018|          31.12.2050|     24656|30114840700000770002|        A|         35|          840|        2|      107|            null|\n",
      "|      15.02.2018|          31.12.2050|     24753|30114124400000004011|        A|         42|          124|       97|      101|            null|\n",
      "|      21.04.2018|          31.12.2050|     25068|30222810400000001100|        P|         34|          643|       74|      115|               Y|\n",
      "+----------------+--------------------+----------+--------------------+---------+-----------+-------------+---------+---------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name_table = \"md_account_d\" #наименование таблицы\n",
    "directory=\"task_2.2/data_deltas\" #путь где хранятся дельты\n",
    "unique_key=[\"ACCOUNT_RK\"] #список полей, являющимся уникальным ключём\n",
    "create_mirror(directory, name_table, unique_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97057b1d-079d-46d4-8b63-1b09a4561b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe79e92-dc18-4ad8-8fb2-ab56b06740f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6e4fcd-8351-4677-8d87-e250a031c7d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
